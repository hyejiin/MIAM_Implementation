{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f47941c-3051-44ef-a528-2f1091461ac2",
   "metadata": {},
   "source": [
    "##  PyTorch implementation of the paper : Multi-view Integration Learning for Irregularly-sampled Clinical Time Series (MIAM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602cfec-b747-4bdd-b8d5-33275aaf5382",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Transformation to MIAM Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb6577-5572-4778-b1d5-0df0fccfe8f0",
   "metadata": {},
   "source": [
    "Using PhysioNet 2012 Challenge dataset, In-Hospital Mortality Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b69e79-6144-4394-8535-1f7f4968da57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62727a37-4a86-4eb3-863c-780fe1503eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 경로 설정\n",
    "base_path = '/media/usr/HDD/Data/EHR/Challenge_2012/'  # 데이터를 저장한 경로\n",
    "sets = ['set-a']\n",
    "\n",
    "\n",
    "# 생리적 데이터 결합\n",
    "physio_dataframes = []\n",
    "\n",
    "for set_name in sets:\n",
    "    set_path = os.path.join(base_path, set_name)\n",
    "    # set_path의 모든 .txt 파일 가져오기\n",
    "    for file_path in glob.glob(os.path.join(set_path, '*.txt')):\n",
    "        df = pd.read_csv(file_path)\n",
    "        record_id = df[df['Parameter'] == 'RecordID']['Value'].values[0]\n",
    "        df['RecordID'] = record_id\n",
    "        df = df[df['Parameter'] != 'RecordID']\n",
    "        physio_dataframes.append(df)\n",
    "\n",
    "\n",
    "# 모든 생리적 데이터를 하나로 결합\n",
    "combined_physio_data = pd.concat(physio_dataframes, ignore_index=True)\n",
    "\n",
    "# Outcomes 데이터 읽기 및 병합\n",
    "outcomes_dataframes = []\n",
    "\n",
    "for set_name in sets:\n",
    "    outcomes_file = f'Outcomes-{set_name[-1]}.txt'  # Outcomes-a.txt, Outcomes-b.txt, Outcomes-c.txt\n",
    "    outcomes_path = os.path.join(base_path, outcomes_file)\n",
    "    outcomes_df = pd.read_csv(outcomes_path)\n",
    "    outcomes_dataframes.append(outcomes_df)\n",
    "\n",
    "# Outcomes 데이터 결합\n",
    "combined_outcomes_data = pd.concat(outcomes_dataframes, ignore_index=True)\n",
    "\n",
    "# 생리적 데이터와 Outcomes 데이터 병합\n",
    "combined_data = pd.merge(combined_physio_data, combined_outcomes_data, on='RecordID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b36b6f-1c92-4ab0-8c71-f261dc216d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.RecordID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aefc3bd-71f4-4368-9303-73830fdd17e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Value</th>\n",
       "      <th>RecordID</th>\n",
       "      <th>SAPS-I</th>\n",
       "      <th>SOFA</th>\n",
       "      <th>Length_of_stay</th>\n",
       "      <th>Survival</th>\n",
       "      <th>In-hospital_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00</td>\n",
       "      <td>Age</td>\n",
       "      <td>46.0</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00</td>\n",
       "      <td>Gender</td>\n",
       "      <td>1.0</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00</td>\n",
       "      <td>Height</td>\n",
       "      <td>175.3</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00</td>\n",
       "      <td>ICUType</td>\n",
       "      <td>2.0</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00</td>\n",
       "      <td>Weight</td>\n",
       "      <td>88.3</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time Parameter  Value  RecordID  SAPS-I  SOFA  Length_of_stay  Survival  \\\n",
       "0  00:00       Age   46.0  138095.0      11     6               5        -1   \n",
       "1  00:00    Gender    1.0  138095.0      11     6               5        -1   \n",
       "2  00:00    Height  175.3  138095.0      11     6               5        -1   \n",
       "3  00:00   ICUType    2.0  138095.0      11     6               5        -1   \n",
       "4  00:00    Weight   88.3  138095.0      11     6               5        -1   \n",
       "\n",
       "   In-hospital_death  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68ca66-c9ab-471a-b3af-53ce05cd8ead",
   "metadata": {},
   "source": [
    "### RecordID, Time, Parameter 기준 중복값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "545bf370-b969-4ee6-8715-d0a827ecd26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 값:\n",
      "         RecordID   Time Parameter  Value\n",
      "31       138095.0  02:44     Urine    0.0\n",
      "32       138095.0  02:44     Urine  500.0\n",
      "500      141128.0  04:30     Urine   80.0\n",
      "501      141128.0  04:30     Urine    0.0\n",
      "763      141128.0  29:30     Urine    0.0\n",
      "...           ...    ...       ...    ...\n",
      "1750015  134985.0  14:15     Urine   60.0\n",
      "1750358  134162.0  03:25     Urine    0.0\n",
      "1750359  134162.0  03:25     Urine   60.0\n",
      "1753381  136164.0  25:02      Temp   37.1\n",
      "1753382  136164.0  25:02      Temp    0.0\n",
      "\n",
      "[7474 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data에서 중복값을 찾기 위한 설정\n",
    "# 먼저 RecordID, Time, Parameter 기준으로 그룹화한 후, 각 그룹 내에서 중복된 값을 찾습니다.\n",
    "\n",
    "# 중복 확인을 위해 'Value' 열을 포함한 데이터프레임을 생성합니다.\n",
    "duplicate_values = combined_data[['RecordID', 'Time', 'Parameter', 'Value']]\n",
    "\n",
    "# 중복된 값 찾기\n",
    "duplicates = duplicate_values[duplicate_values.duplicated(subset=['RecordID', 'Time', 'Parameter'], keep=False)]\n",
    "\n",
    "# 중복값 출력\n",
    "if not duplicates.empty:\n",
    "    print(\"중복된 값:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"중복된 값이 없습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9154faf-37f9-48be-8b6e-de724455f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 값의 변수 목록:\n",
      "['Urine' 'MAP' 'Temp' 'HCT' 'BUN' 'Creatinine' 'Glucose' 'HCO3' 'Mg' 'K'\n",
      " 'Na' 'TroponinT' 'Platelets' 'WBC' 'ALP' 'ALT' 'AST' 'RespRate' 'Albumin'\n",
      " 'Bilirubin']\n"
     ]
    }
   ],
   "source": [
    "# 중복값 찾기\n",
    "duplicates = duplicate_values[duplicate_values.duplicated(subset=['RecordID', 'Time', 'Parameter'], keep=False)]\n",
    "\n",
    "# 중복된 값에서 변수 목록 추출\n",
    "if not duplicates.empty:\n",
    "    unique_parameters = duplicates['Parameter'].unique()\n",
    "    print(\"중복된 값의 변수 목록:\")\n",
    "    print(unique_parameters)\n",
    "else:\n",
    "    print(\"중복된 값이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70703626-c128-4317-a8fb-5aac8297134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정리된 데이터의 크기: (1750236, 9)\n",
      "정리된 데이터 샘플:\n"
     ]
    }
   ],
   "source": [
    "# 중복값 처리: 중복값 중 0인 값을 제외하고, 가장 마지막 관측치를 선택\n",
    "final_data_cleaned = combined_data.copy()\n",
    "\n",
    "# 중복된 값의 인덱스를 찾기\n",
    "duplicates = final_data_cleaned[final_data_cleaned.duplicated(subset=['RecordID', 'Time', 'Parameter'], keep=False)]\n",
    "\n",
    "# 중복값 중 0인 값을 제외\n",
    "duplicates_non_zero = duplicates[duplicates['Value'] != 0]\n",
    "\n",
    "# 마지막 관측치 선택\n",
    "last_observations = duplicates_non_zero.groupby(['RecordID', 'Time', 'Parameter']).last().reset_index()\n",
    "\n",
    "# 원본 데이터에서 0인 값을 제외한 마지막 관측치로 대체\n",
    "final_data_cleaned = final_data_cleaned.drop(duplicates.index)  # 중복값 제거\n",
    "final_data_cleaned = pd.concat([final_data_cleaned, last_observations], ignore_index=True)  # 마지막 관측치 추가\n",
    "\n",
    "# 정리된 데이터 출력\n",
    "print(\"정리된 데이터의 크기:\", final_data_cleaned.shape)\n",
    "print(\"정리된 데이터 샘플:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b274c5c-25d3-46a5-a76a-b6b29100d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_values = ['Age', 'Gender', 'Height', 'ICUType', 'Weight', 'MechVent']\n",
    "final_data_cleaned = final_data_cleaned[~final_data_cleaned['Parameter'].isin(exclude_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da837136-6028-4901-b52a-42b89b05ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_cleaned = final_data_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a194c884-d19c-4a7c-8499-14030360646b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Value</th>\n",
       "      <th>RecordID</th>\n",
       "      <th>SAPS-I</th>\n",
       "      <th>SOFA</th>\n",
       "      <th>Length_of_stay</th>\n",
       "      <th>Survival</th>\n",
       "      <th>In-hospital_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00:35</td>\n",
       "      <td>pH</td>\n",
       "      <td>7.40</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00:35</td>\n",
       "      <td>PaCO2</td>\n",
       "      <td>45.00</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00:35</td>\n",
       "      <td>PaO2</td>\n",
       "      <td>332.00</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00:59</td>\n",
       "      <td>pH</td>\n",
       "      <td>7.44</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00:59</td>\n",
       "      <td>PaCO2</td>\n",
       "      <td>40.00</td>\n",
       "      <td>138095.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time Parameter   Value  RecordID  SAPS-I  SOFA  Length_of_stay  Survival  \\\n",
       "5  00:35        pH    7.40  138095.0      11     6               5        -1   \n",
       "6  00:35     PaCO2   45.00  138095.0      11     6               5        -1   \n",
       "7  00:35      PaO2  332.00  138095.0      11     6               5        -1   \n",
       "8  00:59        pH    7.44  138095.0      11     6               5        -1   \n",
       "9  00:59     PaCO2   40.00  138095.0      11     6               5        -1   \n",
       "\n",
       "   In-hospital_death  \n",
       "5                  0  \n",
       "6                  0  \n",
       "7                  0  \n",
       "8                  0  \n",
       "9                  0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61e93164-7636-4868-9ac2-b01e0afbe83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3997"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_cleaned.RecordID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba069602-65d8-4c2e-95cf-93fb5280cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique variables and count\n",
    "variables = final_data_cleaned['Parameter'].unique()\n",
    "num_variables = len(variables)\n",
    "\n",
    "# Map each parameter to an index for easier array population\n",
    "variable_index = {var: idx for idx, var in enumerate(variables)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76660bbc-1ad3-40d2-b2c3-05168e1ea4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pH': 0,\n",
       " 'PaCO2': 1,\n",
       " 'PaO2': 2,\n",
       " 'Urine': 3,\n",
       " 'DiasABP': 4,\n",
       " 'HR': 5,\n",
       " 'MAP': 6,\n",
       " 'SysABP': 7,\n",
       " 'Temp': 8,\n",
       " 'SaO2': 9,\n",
       " 'Mg': 10,\n",
       " 'Platelets': 11,\n",
       " 'GCS': 12,\n",
       " 'FiO2': 13,\n",
       " 'BUN': 14,\n",
       " 'Creatinine': 15,\n",
       " 'Glucose': 16,\n",
       " 'HCO3': 17,\n",
       " 'HCT': 18,\n",
       " 'K': 19,\n",
       " 'Na': 20,\n",
       " 'WBC': 21,\n",
       " 'NIDiasABP': 22,\n",
       " 'NIMAP': 23,\n",
       " 'NISysABP': 24,\n",
       " 'Lactate': 25,\n",
       " 'TroponinI': 26,\n",
       " 'Bilirubin': 27,\n",
       " 'ALP': 28,\n",
       " 'ALT': 29,\n",
       " 'AST': 30,\n",
       " 'TroponinT': 31,\n",
       " 'RespRate': 32,\n",
       " 'Cholesterol': 33,\n",
       " 'Albumin': 34}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b6e43-61b7-4cde-96f3-65f4c042a58f",
   "metadata": {},
   "source": [
    "### 환자 별 X, M, Delta 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475cdff-b776-48e8-b966-f8e5ac5f877b",
   "metadata": {},
   "source": [
    "- 48시간동안 관찰 및 기록된 데이터 : 일반적으로 한 시간에 한 번 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2197a9e-8fc5-49a9-bc8a-aa20b00a1347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[  7.44,   7.36,   7.36, ...,    nan,    nan,    nan],\n",
       "        [ 40.  ,  50.  ,  46.  , ...,    nan,    nan,    nan],\n",
       "        [198.  , 133.  , 162.  , ...,    nan,    nan,    nan],\n",
       "        ...,\n",
       "        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan, ...,    nan,    nan,    nan]]),\n",
       " 'M': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 'Delta': array([[ 0.,  1.,  1., ..., 35., 36., 37.],\n",
       "        [ 0.,  1.,  1., ..., 39., 40., 41.],\n",
       "        [ 0.,  1.,  1., ..., 39., 40., 41.],\n",
       "        ...,\n",
       "        [ 0.,  1.,  2., ..., 45., 46., 47.],\n",
       "        [ 0.,  1.,  2., ..., 45., 46., 47.],\n",
       "        [ 0.,  1.,  2., ..., 45., 46., 47.]]),\n",
       " 's': array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
       "        13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
       "        26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
       "        39., 40., 41., 42., 43., 44., 45., 46., 47.])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters for time and structure\n",
    "total_time_hours = 48  # Total time in hours (0 to 47 hours)\n",
    "time_step_minutes = 60  # Step size in minutes (1 hour intervals)\n",
    "num_time_steps = total_time_hours  # Number of steps for 48 hours in 1-hour intervals\n",
    "\n",
    "# Get unique variables and count\n",
    "variables = final_data_cleaned['Parameter'].unique()\n",
    "num_variables = len(variables)\n",
    "\n",
    "# Map each parameter to an index for easier array population\n",
    "variable_index = {var: idx for idx, var in enumerate(variables)}\n",
    "final_data_cleaned['Time_minutes'] = pd.to_timedelta(final_data_cleaned['Time'] + ':00').dt.total_seconds() / 60\n",
    "\n",
    "# Prepare storage for each patient's data in a dictionary format\n",
    "all_patient_data = {}  # Dictionary to hold all patient data\n",
    "labels = []  # To store labels for each patient\n",
    "record_id_to_index = []  # To map RecordID to patient index\n",
    "\n",
    "# Process data per patient based on RecordID\n",
    "unique_record_ids = final_data_cleaned['RecordID'].unique()\n",
    "\n",
    "for idx, record_id in enumerate(unique_record_ids):\n",
    "    # Filter data for the current patient\n",
    "    patient_df = final_data_cleaned[final_data_cleaned['RecordID'] == record_id]\n",
    "    \n",
    "    # Initialize X, M, Delta, and s matrices for this patient\n",
    "    X_patient = np.full((num_variables, num_time_steps), np.nan)  # Each row is a variable, columns are time steps\n",
    "    M_patient = np.zeros((num_variables, num_time_steps), dtype=int)  # Masking matrix\n",
    "    Delta_patient = np.zeros((num_variables, num_time_steps))  # Time interval matrix\n",
    "    s_patient = np.arange(num_time_steps) * time_step_minutes / 60  # Timestamps in hours\n",
    "\n",
    "    # Sort patient data by time in ascending order\n",
    "    patient_df = patient_df.sort_values(by=\"Time_minutes\")\n",
    "    \n",
    "    # Populate X and M for each variable at each time step\n",
    "    for _, row in patient_df.iterrows():\n",
    "        time_in_hours = int(row['Time_minutes'] // time_step_minutes)\n",
    "        if time_in_hours < total_time_hours:  # Ensure within 48 hours\n",
    "            var_idx = variable_index[row['Parameter']]\n",
    "            X_patient[var_idx, time_in_hours] = row['Value']  # Assign value to correct variable and time step\n",
    "            M_patient[var_idx, time_in_hours] = 1  # Mark as observed\n",
    "\n",
    "\n",
    "    # Calculate Delta for time intervals\n",
    "    for var_idx in range(num_variables):\n",
    "        last_observed = -1\n",
    "        for t in range(num_time_steps):\n",
    "            if M_patient[var_idx, t] == 1:\n",
    "                if last_observed == -1:\n",
    "                    Delta_patient[var_idx, t] = 0\n",
    "                else:\n",
    "                    Delta_patient[var_idx, t] = s_patient[t] - s_patient[last_observed]\n",
    "                last_observed = t\n",
    "            elif t > 0:\n",
    "                Delta_patient[var_idx, t] = Delta_patient[var_idx, t - 1] + (s_patient[t] - s_patient[t - 1])\n",
    "\n",
    "    # Store matrices in dictionary format for each patient\n",
    "    all_patient_data[record_id] = {\n",
    "        \"X\": X_patient,\n",
    "        \"M\": M_patient,\n",
    "        \"Delta\": Delta_patient,\n",
    "        \"s\": s_patient\n",
    "    }\n",
    "    \n",
    "    # Store labels for this patient\n",
    "    labels.append({\n",
    "        \"RecordID\": record_id,\n",
    "        \"label\": patient_df.iloc[0]['In-hospital_death'],\n",
    "        \"label_2\": patient_df.iloc[0]['Length_of_stay']\n",
    "    })\n",
    "    \n",
    "    # Record mapping of RecordID to patient index\n",
    "    record_id_to_index.append({\"RecordID\": record_id, \"PatientIndex\": idx})\n",
    "\n",
    "# Convert labels and RecordID mapping to DataFrames\n",
    "labels_df = pd.DataFrame(labels)\n",
    "record_id_to_index_df = pd.DataFrame(record_id_to_index)\n",
    "\n",
    "# Return the data structure for the first patient for verification\n",
    "all_patient_data[unique_record_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f050ee6d-acdd-4fcd-8192-9f56c8e41f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[  7.44,   7.36,   7.36, ...,    nan,    nan,    nan],\n",
       "        [ 40.  ,  50.  ,  46.  , ...,    nan,    nan,    nan],\n",
       "        [198.  , 133.  , 162.  , ...,    nan,    nan,    nan],\n",
       "        ...,\n",
       "        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan, ...,    nan,    nan,    nan]]),\n",
       " 'M': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 'Delta': array([[ 0.,  1.,  1., ..., 35., 36., 37.],\n",
       "        [ 0.,  1.,  1., ..., 39., 40., 41.],\n",
       "        [ 0.,  1.,  1., ..., 39., 40., 41.],\n",
       "        ...,\n",
       "        [ 0.,  1.,  2., ..., 45., 46., 47.],\n",
       "        [ 0.,  1.,  2., ..., 45., 46., 47.],\n",
       "        [ 0.,  1.,  2., ..., 45., 46., 47.]]),\n",
       " 's': array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
       "        13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
       "        26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
       "        39., 40., 41., 42., 43., 44., 45., 46., 47.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_patient_data = {idx: all_patient_data[record['RecordID']] for idx, record in enumerate(record_id_to_index)}\n",
    "indexed_patient_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be4d7a20-f3fa-4a4d-92c3-739fdaadb4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>PatientIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138095.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141128.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134239.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135513.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139323.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>137871.0</td>\n",
       "      <td>3992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>134420.0</td>\n",
       "      <td>3993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>135107.0</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>136164.0</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>133215.0</td>\n",
       "      <td>3996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RecordID  PatientIndex\n",
       "0     138095.0             0\n",
       "1     141128.0             1\n",
       "2     134239.0             2\n",
       "3     135513.0             3\n",
       "4     139323.0             4\n",
       "...        ...           ...\n",
       "3992  137871.0          3992\n",
       "3993  134420.0          3993\n",
       "3994  135107.0          3994\n",
       "3995  136164.0          3995\n",
       "3996  133215.0          3996\n",
       "\n",
       "[3997 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_id_to_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be92e88b-4edb-45c2-9026-50b019bb2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 48)\n",
      "(35, 48)\n",
      "(35, 48)\n",
      "(48,)\n"
     ]
    }
   ],
   "source": [
    "print(indexed_patient_data[0]['X'].shape)\n",
    "print(indexed_patient_data[0]['M'].shape)\n",
    "print(indexed_patient_data[0]['Delta'].shape)\n",
    "print(indexed_patient_data[0]['s'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c522fd-6d57-43b2-91fb-2060e1daa72f",
   "metadata": {},
   "source": [
    "### 논문 input 과 동일한 shape 으로 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952085c5-4ac1-48ff-8e75-8a3fa334a4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3997, 48, 35), (3997, 48, 35), (3997, 48, 35), (3997, 48))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize arrays to store all patients' X, M, Delta, s data in a structured format\n",
    "num_patients = len(unique_record_ids)\n",
    "\n",
    "# Prepare lists to hold each array for all patients\n",
    "X = []\n",
    "M = []\n",
    "Delta = []\n",
    "s = []\n",
    "\n",
    "# Loop through each patient to populate the arrays\n",
    "for record_id in unique_record_ids:\n",
    "    patient_data = all_patient_data[record_id]\n",
    "    X.append(patient_data['X'])\n",
    "    M.append(patient_data['M'])\n",
    "    Delta.append(patient_data['Delta'])\n",
    "    s.append(patient_data['s'])\n",
    "\n",
    "# Convert lists to arrays for structured storage if needed\n",
    "X = np.array(X)     # Shape: (num_patients, num_variables, num_time_steps)\n",
    "M = np.array(M)     # Shape: (num_patients, num_variables, num_time_steps)\n",
    "Delta = np.array(Delta)  # Shape: (num_patients, num_variables, num_time_steps)\n",
    "s = np.array(s)     # Shape: (num_patients, num_time_steps)\n",
    "\n",
    "# Transpose arrays to match the desired shape: (num_patients, num_time_steps, num_variables)\n",
    "X = np.transpose(X, (0, 2, 1))     # Shape: (num_patients, num_time_steps, num_variables)\n",
    "M = np.transpose(M, (0, 2, 1))     # Shape: (num_patients, num_time_steps, num_variables)\n",
    "Delta = np.transpose(Delta, (0, 2, 1))  # Shape: (num_patients, num_time_steps, num_variables)\n",
    "\n",
    "\n",
    "# Confirm the shape of each array for verification\n",
    "X.shape, M.shape, Delta.shape, s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae9a7a-77ce-4f66-956e-864f49547a3a",
   "metadata": {},
   "source": [
    "### kfold 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5c9073-6d0c-45bc-a818-aaa9e08d92d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kfold_X[0][0] (train)': (2396, 48, 35),\n",
       " 'kfold_X[0][1] (validate)': (800, 48, 35),\n",
       " 'kfold_X[0][2] (test)': (801, 48, 35),\n",
       " 'kfold_label[0][0] (train label)': (2396,),\n",
       " 'kfold_label[0][1] (validate label)': (800,),\n",
       " 'kfold_label[0][2] (test label)': (801,)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "# Prepare labels for stratified splitting\n",
    "label = labels_df['label'].values\n",
    "label_2 = labels_df['label_2'].values\n",
    "\n",
    "# Adjust label to have an 8:2 ratio for stratified splitting\n",
    "# Assuming binary classification, we can convert label to categorical with 0 (negative) and 1 (positive)\n",
    "# We will manually create the stratification based on this label\n",
    "positive_indices = np.where(label == 1)[0]\n",
    "negative_indices = np.where(label == 0)[0]\n",
    "\n",
    "# Calculate how many positive and negative samples to include in each fold\n",
    "num_positive = int(len(positive_indices) * 0.8)  # 80% positive for training\n",
    "num_negative = int(len(negative_indices) * 0.8)  # 80% negative for training\n",
    "\n",
    "# Initialize the k-fold split containers\n",
    "num_folds = 4\n",
    "kfold_X = [[] for _ in range(num_folds)]\n",
    "kfold_M = [[] for _ in range(num_folds)]\n",
    "kfold_Delta = [[] for _ in range(num_folds)]\n",
    "kfold_s = [[] for _ in range(num_folds)]\n",
    "kfold_label = [[] for _ in range(num_folds)]\n",
    "kfold_label_2 = [[] for _ in range(num_folds)]\n",
    "\n",
    "# StratifiedShuffleSplit to create train, validate, and test splits\n",
    "outer_splitter = StratifiedShuffleSplit(n_splits=num_folds, test_size=0.2002, random_state=128) \n",
    "\n",
    "for fold_idx, (train_val_index, test_index) in enumerate(outer_splitter.split(np.zeros(len(label)), label)):\n",
    "    # Split the remaining 80% into train (70%) and validate (10%)\n",
    "    inner_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2002 / 0.7998, random_state=128)  \n",
    "    train_index, val_index = next(inner_splitter.split(np.zeros(len(train_val_index)), label[train_val_index]))\n",
    "\n",
    "    # Map indices back to the original array\n",
    "    train_index = train_val_index[train_index]\n",
    "    val_index = train_val_index[val_index]\n",
    "\n",
    "    # Assign train, validate, and test data for each fold\n",
    "    kfold_X[fold_idx] = [X[train_index], X[val_index], X[test_index]]\n",
    "    kfold_M[fold_idx] = [M[train_index], M[val_index], M[test_index]]\n",
    "    kfold_Delta[fold_idx] = [Delta[train_index], Delta[val_index], Delta[test_index]]\n",
    "    kfold_s[fold_idx] = [s[train_index], s[val_index], s[test_index]]\n",
    "    kfold_label[fold_idx] = [label[train_index], label[val_index], label[test_index]]\n",
    "    kfold_label_2[fold_idx] = [label_2[train_index], label_2[val_index], label_2[test_index]]\n",
    "\n",
    "# Output structure of the first fold to confirm\n",
    "{\n",
    "    \"kfold_X[0][0] (train)\": kfold_X[0][0].shape,\n",
    "    \"kfold_X[0][1] (validate)\": kfold_X[0][1].shape,\n",
    "    \"kfold_X[0][2] (test)\": kfold_X[0][2].shape,\n",
    "    \"kfold_label[0][0] (train label)\": kfold_label[0][0].shape,\n",
    "    \"kfold_label[0][1] (validate label)\": kfold_label[0][1].shape,\n",
    "    \"kfold_label[0][2] (test label)\": kfold_label[0][2].shape\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "773320b0-5e44-4831-87ef-8593f7716ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'train_missing_rate': 0.8059839315525876,\n",
       "  'val_missing_rate': 0.8049375,\n",
       "  'test_missing_rate': 0.8029895666131621,\n",
       "  'train_positive_ratio': 0.13856427378964942,\n",
       "  'val_positive_ratio': 0.13875,\n",
       "  'test_positive_ratio': 0.13857677902621723,\n",
       "  'train_label_2_negative_ratio': 0.014607679465776294,\n",
       "  'val_label_2_negative_ratio': 0.0125,\n",
       "  'test_label_2_negative_ratio': 0.0149812734082397},\n",
       " 1: {'train_missing_rate': 0.8047112250576357,\n",
       "  'val_missing_rate': 0.8062462797619048,\n",
       "  'test_missing_rate': 0.805489417989418,\n",
       "  'train_positive_ratio': 0.13856427378964942,\n",
       "  'val_positive_ratio': 0.13875,\n",
       "  'test_positive_ratio': 0.13857677902621723,\n",
       "  'train_label_2_negative_ratio': 0.013772954924874792,\n",
       "  'val_label_2_negative_ratio': 0.0175,\n",
       "  'test_label_2_negative_ratio': 0.012484394506866416},\n",
       " 2: {'train_missing_rate': 0.8057037025995707,\n",
       "  'val_missing_rate': 0.8053236607142857,\n",
       "  'test_missing_rate': 0.8034421259140361,\n",
       "  'train_positive_ratio': 0.13856427378964942,\n",
       "  'val_positive_ratio': 0.13875,\n",
       "  'test_positive_ratio': 0.13857677902621723,\n",
       "  'train_label_2_negative_ratio': 0.015442404006677797,\n",
       "  'val_label_2_negative_ratio': 0.015,\n",
       "  'test_label_2_negative_ratio': 0.009987515605493134},\n",
       " 3: {'train_missing_rate': 0.805652277605533,\n",
       "  'val_missing_rate': 0.8057269345238095,\n",
       "  'test_missing_rate': 0.8031931811426194,\n",
       "  'train_positive_ratio': 0.13856427378964942,\n",
       "  'val_positive_ratio': 0.13875,\n",
       "  'test_positive_ratio': 0.13857677902621723,\n",
       "  'train_label_2_negative_ratio': 0.015442404006677797,\n",
       "  'val_label_2_negative_ratio': 0.01125,\n",
       "  'test_label_2_negative_ratio': 0.01373283395755306}}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a structure to hold missing rates and label distributions for each fold\n",
    "fold_summary = {}\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    # Extract the data for the current fold\n",
    "    train_data = kfold_X[fold_idx][0]\n",
    "    val_data = kfold_X[fold_idx][1]\n",
    "    test_data = kfold_X[fold_idx][2]\n",
    "    \n",
    "    train_labels = kfold_label[fold_idx][0]\n",
    "    val_labels = kfold_label[fold_idx][1]\n",
    "    test_labels = kfold_label[fold_idx][2]\n",
    "\n",
    "    # Calculate missing rates for train, validate, and test datasets\n",
    "    train_missing_rate = np.mean(np.isnan(train_data))\n",
    "    val_missing_rate = np.mean(np.isnan(val_data))\n",
    "    test_missing_rate = np.mean(np.isnan(test_data))\n",
    "\n",
    "    # Calculate label distributions for train, validate, and test datasets\n",
    "    train_positive_ratio = np.sum(train_labels == 1) / len(train_labels)\n",
    "    val_positive_ratio = np.sum(val_labels == 1) / len(val_labels)\n",
    "    test_positive_ratio = np.sum(test_labels == 1) / len(test_labels)\n",
    "\n",
    "    # Calculate the ratio of -1 in label_2 for each set\n",
    "    train_label_2_negative_ratio = np.sum(kfold_label_2[fold_idx][0] == -1) / len(kfold_label_2[fold_idx][0])\n",
    "    val_label_2_negative_ratio = np.sum(kfold_label_2[fold_idx][1] == -1) / len(kfold_label_2[fold_idx][1])\n",
    "    test_label_2_negative_ratio = np.sum(kfold_label_2[fold_idx][2] == -1) / len(kfold_label_2[fold_idx][2])\n",
    "\n",
    "    # Store the results for the current fold\n",
    "    fold_summary[fold_idx] = {\n",
    "        \"train_missing_rate\": train_missing_rate,\n",
    "        \"val_missing_rate\": val_missing_rate,\n",
    "        \"test_missing_rate\": test_missing_rate,\n",
    "        \"train_positive_ratio\": train_positive_ratio,\n",
    "        \"val_positive_ratio\": val_positive_ratio,\n",
    "        \"test_positive_ratio\": test_positive_ratio,\n",
    "        \"train_label_2_negative_ratio\": train_label_2_negative_ratio,\n",
    "        \"val_label_2_negative_ratio\": val_label_2_negative_ratio,\n",
    "        \"test_label_2_negative_ratio\": test_label_2_negative_ratio,\n",
    "    }\n",
    "\n",
    "# Display the summary for the first fold\n",
    "fold_summary  # Showing for fold 0, can be changed to other fold indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364720f7-f299-4252-8866-bb17ab3a947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kfold_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'kfold_X': kfold_X,\n",
    "        'kfold_M': kfold_M,\n",
    "        'kfold_Delta': kfold_Delta,\n",
    "        'kfold_s': kfold_s,\n",
    "        'kfold_label': kfold_label,\n",
    "        'kfold_label_2': kfold_label_2\n",
    "    }, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5f5da-f624-4536-b421-6676bfffe08e",
   "metadata": {},
   "source": [
    "### Task :  In-Hospital Mortality Prediction (Binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a67c9-33b4-4ca9-8bd3-a91c02451619",
   "metadata": {},
   "source": [
    "Need to import\n",
    "- model.py\n",
    "- help_physionet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39822359-3651-49c2-97b4-f87cc4c48fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout zero, relu\n",
      "Using GPU ID 0 if available, else CPU\n",
      "Assigned device: cuda:0\n",
      "focal(y): 0 , mse(x): 1\n",
      "---------------\n",
      "MIAM\n",
      "Dataset :/media/usr/HDD/hyejin\n",
      "---------------\n",
      "TRAINING PARAMETER\n",
      "Learning Rate : 0.0005\n",
      "LR decay : 0.1\n",
      "Batch Size : 64\n",
      "lambda1 : 0.0005\n",
      "---------------\n",
      "Transformer Setup\n",
      "hidden_dim : 64\n",
      "FFN_dim : 64\n",
      "num_heads : 4\n",
      "num_stacks : 1\n",
      "---------------\n",
      "Loss Setup\n",
      "cls:0, reg:1, imp:1\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import argparse\n",
    "import warnings\n",
    "import random\n",
    "from help_physionet import *\n",
    "from models import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torchstat import stat\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"GEVENT_SUPPORT\"] = \"True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "torch.backends.cudnn.enabled = False\n",
    "JOBLIB_MULTIPROCESSING=1\n",
    "\n",
    "# Define Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", help=\"which dataset to use\", type=str, default='physionet')\n",
    "parser.add_argument('--fold_num', type=int, default=0)\n",
    "parser.add_argument('--l1', type=float, default=5e-4)\n",
    "parser.add_argument('--w_decay', type=float, default=5e-3)#1e-3)\n",
    "parser.add_argument('--lr', type=float, default=5e-4)#5e-3)\n",
    "parser.add_argument('--lr_decay', type=int, default=15)\n",
    "parser.add_argument('--lr_ratio', type=float, default=0.1)\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "\n",
    "\n",
    "print('dropout zero, relu')\n",
    "args, unknown = parser.parse_known_args() \n",
    "dataset = args.dataset\n",
    "fold_num = args.fold_num\n",
    "l1 = args.l1\n",
    "w_decay = args.w_decay\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "lr_decay = args.lr_decay\n",
    "lr_ratio = args.lr_ratio\n",
    "\n",
    "# Set the GPU configuration\n",
    "device_number = args.gpu_id\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(device_number)\n",
    "dev_allo = f\"cuda:{device_number}\" if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(dev_allo)\n",
    "\n",
    "print(f'Using GPU ID {device_number} if available, else CPU')\n",
    "print(f'Assigned device: {device}')\n",
    "\n",
    "# Load Kfold dataset\n",
    "data_dir = '/media/usr/HDD/hyejin'\n",
    "# kfold_data = np.load(open(data_dir + 'kfold_data_35.p', 'rb'), mmap_mode='r', allow_pickle=True)\n",
    "# kfold_mask = np.load(open(data_dir + 'kfold_mask_35.p', 'rb'), mmap_mode='r', allow_pickle=True)\n",
    "# kfold_label = np.load(open(data_dir + 'kfold_label_35.p', 'rb'), mmap_mode='r', allow_pickle=True)\n",
    "# kfold_label2 = np.load(open('/home/yrlee/irregular_data/kfold_los_label.p', 'rb'), mmap_mode='r', allow_pickle=True)\n",
    "# kfold_times = np.load(open(data_dir + 'kfold_times_35.p', 'rb'), mmap_mode='r', allow_pickle=True)\n",
    "\n",
    "with open('kfold_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "kfold_data = data['kfold_X']\n",
    "kfold_mask = data['kfold_M']\n",
    "kfold_times = data['kfold_s']\n",
    "kfold_label = data['kfold_label']\n",
    "kfold_label2 = data['kfold_label_2']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "n_epochs = 60\n",
    "alpha = 9\n",
    "gamma = 0.15\n",
    "beta = 0.1\n",
    "delta = 11\n",
    "# Loss rates\n",
    "lambda_1 = 0\n",
    "lambda_2 = 1\n",
    "lambda_3 = 1\n",
    "print('focal(y):', str(lambda_1), ', mse(x):', str(lambda_2))\n",
    "KFold = len(kfold_data)\n",
    "\n",
    "# Network architecture\n",
    "max_length = kfold_data[0][0].shape[1]\n",
    "input_dim = kfold_data[0][0].shape[2]\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 64\n",
    "num_stacks = 1\n",
    "num_heads = 4\n",
    "\n",
    "# Seed\n",
    "manualSeed = 128\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "# kfold performance\n",
    "kfold_mse = []\n",
    "kfold_mae = []\n",
    "kfold_acc = []\n",
    "kfold_balacc = []\n",
    "kfold_auc = []\n",
    "kfold_auprc = []\n",
    "kfold_sen = []\n",
    "kfold_spec = []\n",
    "kfold_precision = []\n",
    "kfold_recall = []\n",
    "kfold_f1_score_pr = []\n",
    "kfold_f2_score_pr = []\n",
    "\n",
    "\n",
    "def switch(fold_num):\n",
    "    return {0: range(0, 1),\n",
    "            1: range(1, 2),\n",
    "            2: range(2, 3),\n",
    "            3: range(3, 4),\n",
    "            4: range(4, 5)}[fold_num]\n",
    "\n",
    "\n",
    "# Create Directories\n",
    "log_dir = './log/' + str(datetime.datetime.now().strftime('%y%m%d')) + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    os.chmod(log_dir, mode=0o777)\n",
    "dir = log_dir + 'observation_mask_multi_encoder_' + str(datetime.datetime.now().strftime('%H.%M.%S')) + '/'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "    os.makedirs(dir + 'model/')\n",
    "    os.makedirs(dir + 'tflog/')\n",
    "    for k in range(KFold):\n",
    "        os.makedirs(dir + 'model/' + str(k) + '/')\n",
    "\n",
    "# TensorBoard Logging Setup\n",
    "writer = SummaryWriter(log_dir=dir + 'tflog')\n",
    "\n",
    "# Text Logging\n",
    "f = open(dir + 'log.txt', 'a')\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'MIAM')\n",
    "writelog(f, 'Dataset :' + str(data_dir))\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'TRAINING PARAMETER')\n",
    "writelog(f, 'Learning Rate : ' + str(lr))\n",
    "writelog(f, 'LR decay : '+ str(lr_ratio))\n",
    "writelog(f, 'Batch Size : ' + str(batch_size))\n",
    "writelog(f, 'lambda1 : ' + str(l1))\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'Transformer Setup')\n",
    "writelog(f, 'hidden_dim : ' + str(d_model))\n",
    "writelog(f, 'FFN_dim : ' + str(d_ff))\n",
    "writelog(f, 'num_heads : ' + str(num_heads))\n",
    "writelog(f, 'num_stacks : ' + str(num_stacks))\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'Loss Setup')\n",
    "writelog(f, 'cls:'+ str(lambda_1) + ', reg:' + str(lambda_2) +', imp:'+ str(lambda_3))\n",
    "writelog(f, '---------------')\n",
    "\n",
    "def train(epoch, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x = data['values'].to(device)  # Batch x Time x Variable\n",
    "        m = data['masks'].to(device)  # Batch x Time x Variable\n",
    "        deltas = data['deltas'].to(device)  # Batch x Time x Variable\n",
    "        times = data['times'].to(device)  # Batch x Time x Variable\n",
    "        y = data['labels'].to(device)\n",
    "\n",
    "        attn_mask = deltas.data.eq(0)[:, :, 0]\n",
    "        attn_mask[:, 0] = 0\n",
    "\n",
    "        # Zero Grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # model\n",
    "        output, out = model(x, m, times, deltas, attn_mask)\n",
    "\n",
    "        # Calculate and store the loss\n",
    "        loss_a = criterion_focal(model, output, y)\n",
    "        loss_b = criterion_mse(out, x)\n",
    "        loss = beta*loss_a + delta*loss_b\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = train_loss / n_batches\n",
    "    writelog(f, 'Train loss : ' + str(train_loss))\n",
    "\n",
    "\n",
    "def test(phase, epoch, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    n_batches = 0.0\n",
    "\n",
    "    y_gts = np.array([]).reshape(0)\n",
    "    y_preds = np.array([]).reshape(0)\n",
    "    y_scores = np.array([]).reshape(0)\n",
    "\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        x = data['values'].to(device)  # Batch x Time x Variable\n",
    "        m = data['masks'].to(device)  # Batch x Time x Variable\n",
    "        deltas = data['deltas'].to(device)  # Batch x Time x Variable\n",
    "        times = data['times'].to(device)  # Batch x Time x Variable\n",
    "        y = data['labels'].to(device)\n",
    "\n",
    "        attn_mask = deltas.data.eq(0)[:, :, 0]\n",
    "        attn_mask[:, 0] = 0\n",
    "\n",
    "        y_gts = np.hstack([y_gts, y.to('cpu').detach().numpy().flatten()]) #physionet\n",
    "\n",
    "        # model\n",
    "        output, out = model(x, m, times, deltas, attn_mask)\n",
    "\n",
    "        # Calculate and store the loss\n",
    "        loss_a = criterion_focal(model, output, y)\n",
    "        loss_b = criterion_mse(out, x)\n",
    "        loss = loss_a #beta*loss_a + delta*loss_b\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        y_score = output\n",
    "        y_pred = np.round(y_score.to('cpu').detach().numpy())\n",
    "        y_score = y_score.to('cpu').detach().numpy()\n",
    "        y_preds = np.hstack([y_preds, y_pred])\n",
    "        y_scores = np.hstack([y_scores, y_score])\n",
    "\n",
    "        n_batches += 1\n",
    "\n",
    "    # Averaging the loss\n",
    "    test_loss /= n_batches\n",
    "    writelog(f, 'Test loss : ' + str(test_loss))\n",
    "\n",
    "    auc, auprc, acc, balacc, sen, spec, prec, recall = calculate_performance(y_gts, y_scores, y_preds)\n",
    "\n",
    "    writelog(f, 'AUC : ' + str(auc))\n",
    "    writelog(f, 'AUC PRC : ' + str(auprc))\n",
    "    writelog(f, 'Accuracy : ' + str(acc))\n",
    "    writelog(f, 'BalACC : ' + str(balacc))\n",
    "    writelog(f, 'Sensitivity : ' + str(sen))\n",
    "    writelog(f, 'Specificity : ' + str(spec))\n",
    "    writelog(f, 'Precision : ' + str(prec))\n",
    "    writelog(f, 'Recall : ' + str(recall))\n",
    "\n",
    "        # TensorBoard Logging\n",
    "    writer.add_scalars(f'Metrics/{phase}', {\n",
    "        'balacc': balacc,\n",
    "        'auc': auc,\n",
    "        'auc_prc': auprc,\n",
    "        'sens': sen,\n",
    "        'spec': spec,\n",
    "        'precision': prec,\n",
    "        'recall': recall\n",
    "    }, epoch)\n",
    "\n",
    "    return auc, auprc, acc, balacc, sen, spec, prec, recall\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbe8c5-9bf8-4700-9e62-bf249d168700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def train(epoch, train_loader):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     n_batches = 0\n",
    "\n",
    "#     for batch_idx, data in enumerate(train_loader):\n",
    "#         x = data['values'].to(device)  # Batch x Time x Variable\n",
    "#         m = data['masks'].to(device)  # Batch x Time x Variable\n",
    "#         deltas = data['deltas'].to(device)  # Batch x Time x Variable\n",
    "#         times = data['times'].to(device)  # Batch x Time x Variable\n",
    "#         y = data['labels'].to(device)\n",
    "#         y1 = data['los_labels'].to(device)\n",
    "\n",
    "#         attn_mask = deltas.data.eq(0)[:, :, 0]\n",
    "#         attn_mask[:, 0] = 0\n",
    "\n",
    "#         # Zero Grad\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Model forward pass\n",
    "#         los_out, y_hat, out = model(x, m, times, deltas, attn_mask)\n",
    "# #         print(\"Input x:\", x)\n",
    "# #         print(\"Target y:\", y)\n",
    "# #         print(\"Mask m:\", m)\n",
    "# #         print(\"Times:\", times)\n",
    "# #         print(\"Deltas:\", deltas)\n",
    "        \n",
    "# #         print(\"y_hat (model output):\", y_hat)  # 이곳에서 모델의 출력 확인\n",
    "# #         print(\"y (targets):\", y)  # 타겟 값 확인\n",
    "        \n",
    "#         loss_los = criterion_mse(los_out, y1)\n",
    "#         loss_cls = criterion_focal(model, y_hat, y)\n",
    "#         loss_imp = criterion_mse(out, x)\n",
    "#         loss = lambda_1 * loss_cls + lambda_2 * loss_los + lambda_3 * loss_imp\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         # Backward Propagation\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update the weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         n_batches += 1\n",
    "\n",
    "#     train_loss = train_loss / n_batches\n",
    "#     writelog(f, 'Train loss : ' + str(train_loss))\n",
    "#     writer.add_scalar('Loss/train', train_loss, epoch)  # TensorBoard logging for training loss\n",
    "\n",
    "\n",
    "# def test(phase, epoch, test_loader):\n",
    "#     model.eval()\n",
    "#     # test_lrp_X = []\n",
    "#     # test_lrp_M = []\n",
    "#     # test_lrp_D = []\n",
    "#     test_loss = 0.0\n",
    "#     n_batches = 0.0\n",
    "\n",
    "#     y_gts = np.array([]).reshape(0)\n",
    "#     y_gt = np.array([]).reshape(0)\n",
    "#     y_preds = np.array([]).reshape(0)\n",
    "#     y_scores = np.array([]).reshape(0)\n",
    "#     y_prd = np.array([]).reshape(0)\n",
    "#     y_scs = np.array([]).reshape(0)\n",
    "\n",
    "#     for batch_idx, data in enumerate(test_loader):\n",
    "#         x = data['values'].to(device)\n",
    "#         m = data['masks'].to(device)\n",
    "#         deltas = data['deltas'].to(device)\n",
    "#         times = data['times'].to(device)\n",
    "#         y = data['labels'].to(device)\n",
    "#         y1 = data['los_labels'].to(device)\n",
    "\n",
    "#         attn_mask = deltas.data.eq(0)[:, :, 0]\n",
    "#         attn_mask[:, 0] = 0\n",
    "\n",
    "#         y_gts = np.hstack([y_gts, y.to('cpu').detach().numpy().flatten()])\n",
    "#         y_gt = np.hstack([y_gt, y1.to('cpu').detach().numpy().flatten()])\n",
    "\n",
    "#         # Model forward pass\n",
    "#         los_out, y_hat, out = model(x, m, times, deltas, attn_mask)\n",
    "\n",
    "#         # LRP score\n",
    "#         # model_dict = model.state_dict()\n",
    "#         # lrp_X, lrp_M, lrp_D = model.backward_lrp(y_hat, model_dict)\n",
    "#         # test_lrp_X.append(lrp_X)\n",
    "#         # test_lrp_M.append(lrp_M)\n",
    "#         # test_lrp_D.append(lrp_D)\n",
    "\n",
    "#         # Calculate and store the loss\n",
    "#         loss_los = criterion_mse(los_out, y1)\n",
    "#         loss_cls = criterion_focal(model, y_hat, y)\n",
    "#         loss_imp = criterion_mse(out, x)\n",
    "#         loss = lambda_1 * loss_cls + lambda_2 * loss_los + lambda_3 * loss_imp\n",
    "\n",
    "#         test_loss += loss.item()\n",
    "#         n_batches += 1\n",
    "\n",
    "#         y_score = y_hat\n",
    "#         y_pred = np.round(y_score.to('cpu').detach().numpy())\n",
    "#         y_score = y_score.to('cpu').detach().numpy()\n",
    "#         y_preds = np.hstack([y_preds, y_pred])\n",
    "#         y_scores = np.hstack([y_scores, y_score])\n",
    "\n",
    "#         y_sc = los_out\n",
    "#         y_pr = np.round(y_sc.to('cpu').detach().numpy())\n",
    "#         y_sc = y_sc.to('cpu').detach().numpy()\n",
    "#         y_prd = np.hstack([y_prd, y_pr])\n",
    "#         y_scs = np.hstack([y_scs, y_sc])\n",
    "\n",
    "#     # Averaging the loss\n",
    "#     test_loss /= n_batches\n",
    "#     writelog(f, 'Test loss : ' + str(test_loss))\n",
    "#     writer.add_scalar(f'Loss/{phase}', test_loss, epoch)  # TensorBoard logging for test loss\n",
    "\n",
    "#     rmse = np.sqrt(mean_squared_error(y_gt, y_scs))\n",
    "#     mae = mean_absolute_error(y_gt, y_scs)\n",
    "#     auc, auprc, acc, balacc, sen, spec, prec, recall = calculate_performance(y_gts, y_scores, y_preds)\n",
    "\n",
    "#     # Log other metrics to console and TensorBoard\n",
    "#     writelog(f, f'{phase} - AUC : {auc}')\n",
    "#     writelog(f, f'{phase} - AUC PRC : {auprc}')\n",
    "#     writelog(f, f'{phase} - Accuracy : {acc}')\n",
    "#     writelog(f, f'{phase} - BalACC : {balacc}')\n",
    "#     writelog(f, f'{phase} - Sensitivity : {sen}')\n",
    "#     writelog(f, f'{phase} - Specificity : {spec}')\n",
    "#     writelog(f, f'{phase} - Precision : {prec}')\n",
    "#     writelog(f, f'{phase} - Recall : {recall}')\n",
    "#     writelog(f, f'{phase} - RMSE : {rmse}')\n",
    "#     writelog(f, f'{phase} - MAE : {mae}')\n",
    "\n",
    "#     # TensorBoard Logging\n",
    "#     writer.add_scalars(f'Metrics/{phase}', {\n",
    "#         'rmse': rmse,\n",
    "#         'mae': mae,\n",
    "#         'balacc': balacc,\n",
    "#         'auc': auc,\n",
    "#         'auc_prc': auprc,\n",
    "#         'sens': sen,\n",
    "#         'spec': spec,\n",
    "#         'precision': prec,\n",
    "#         'recall': recall\n",
    "#     }, epoch)\n",
    "\n",
    "#     return rmse, mae, auc, auprc, acc, balacc, sen, spec, prec, recall\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7694ead-3ca6-470e-ad3b-cac91b45514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# KFold 반복 루프\n",
    "for k in range(KFold):\n",
    "    writelog(f, 'FOLD ' + str(k))\n",
    "\n",
    "    # TensorBoard Logging을 위한 SummaryWriter 설정\n",
    "    writer_train = SummaryWriter(log_dir=dir + f'tflog/kfold_{k}/train')\n",
    "    writer_valid = SummaryWriter(log_dir=dir + f'tflog/kfold_{k}/valid')\n",
    "    writer_test = SummaryWriter(log_dir=dir + f'tflog/kfold_{k}/test')\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    train_data = kfold_data[k][0]\n",
    "    train_mask = kfold_mask[k][0]\n",
    "    tr_miss_idx = np.where(train_mask == 0)\n",
    "    train_data[tr_miss_idx] = 0\n",
    "    train_label = kfold_label[k][0]\n",
    "    train_label2 = kfold_label2[k][0]\n",
    "    train_time = kfold_times[k][0]\n",
    "\n",
    "    valid_data = kfold_data[k][1]\n",
    "    valid_mask = kfold_mask[k][1]\n",
    "    val_miss_idx = np.where(valid_mask == 0)\n",
    "    valid_data[val_miss_idx] = 0\n",
    "    valid_label = kfold_label[k][1]\n",
    "    valid_label2 = kfold_label2[k][1]\n",
    "    valid_time = kfold_times[k][1]\n",
    "\n",
    "    test_data = kfold_data[k][2]\n",
    "    test_mask = kfold_mask[k][2]\n",
    "    ts_miss_idx = np.where(test_mask == 0)\n",
    "    test_data[ts_miss_idx] = 0\n",
    "    test_label = kfold_label[k][2]\n",
    "    test_label2 = kfold_label2[k][2]\n",
    "    test_time = kfold_times[k][2]\n",
    "    \n",
    "   \n",
    "    # Winsorization (2nd-98th percentile)\n",
    "    writelog(f, 'Winsorization')\n",
    "    train_data = Winsorize(train_data)\n",
    "    valid_data = Winsorize(valid_data)\n",
    "    test_data = Winsorize(test_data)\n",
    "    \n",
    "\n",
    "    # # Normalization\n",
    "    writelog(f, 'Normalization')\n",
    "    train_data, mean_set, std_set = normalize(train_data, train_mask, [], [])\n",
    "    valid_data, m, s = normalize(valid_data, valid_mask, mean_set, std_set)\n",
    "    test_data, m, s = normalize(test_data, test_mask, mean_set, std_set)\n",
    "    \n",
    "    \n",
    "    test_data_zero = test_data.copy()\n",
    "    test_data_zero[ts_miss_idx] = 0  # zero imputation\n",
    "    test_ms_data_zero, test_data_zero, test_msk= random_mask(test_data_zero)\n",
    "\n",
    "\n",
    "    # 데이터 로더 정의\n",
    "    train_loader = sample_loader('train', k, train_data, train_mask, train_label, train_label2, train_time, batch_size, ZeroImpute=True)\n",
    "    valid_loader = sample_loader('valid', k, valid_data, valid_mask, valid_label, valid_label2, valid_time, batch_size, ZeroImpute=True)\n",
    "    test_loader =  msk_sample_loader('test', k, test_data, test_mask, test_ms_data_zero, test_msk, test_label, test_label2, test_time, batch_size, ZeroImpute=True)\n",
    "   \n",
    "\n",
    "    # 모델 및 옵티마이저 정의\n",
    "    criterion_focal = FocalLoss(l1, device, gamma=gamma, alpha=alpha, logits=False).to(device)\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    model = Multi_Duration_Pipeline_Residual(input_dim, d_model, d_ff, num_stacks, num_heads, max_length, n_iter=num_stacks).to(device)\n",
    "    \n",
    "    optimizer = RAdam(list(model.parameters()), lr=lr, weight_decay=w_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay, gamma=lr_ratio)\n",
    "\n",
    "    # Best Validation AUC 초기화\n",
    "    bestValidAUC = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # 훈련, 검증, 테스트 루프\n",
    "    for epoch in range(n_epochs):\n",
    "        writelog(f, '------ Epoch ' + str(epoch))\n",
    "\n",
    "        writelog(f, 'Training')\n",
    "        train(epoch, train_loader)\n",
    "\n",
    "        writelog(f, 'Validation')\n",
    "        #rmse, mae, \n",
    "        auc, auprc, acc, balacc, sen, spec, prec, recall = test('valid', epoch, valid_loader)\n",
    "\n",
    "        # 최적 AUC 모델 저장\n",
    "        if auc > bestValidAUC:\n",
    "            torch.save(model.state_dict(), dir + f'model/{k}/{epoch}_self_attention.pt')\n",
    "            writelog(f, 'Best validation AUC found! Validation AUC : ' + str(auc))\n",
    "            bestValidAUC = auc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        writelog(f, 'Test')\n",
    "        #rmse, mae, \n",
    "        auc, auprc, acc, balacc, sen, spec, prec, recall = test('test', epoch, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        # TensorBoard에 성능 기록\n",
    "        writer_train.add_scalar('AUC/train', auc, epoch)\n",
    "        writer_valid.add_scalar('AUC/valid', auc, epoch)\n",
    "        writer_test.add_scalar('AUC/test', auc, epoch)\n",
    "\n",
    "    # Best Validation 모델 로드 및 최종 테스트\n",
    "    model.load_state_dict(torch.load(dir + f'model/{k}/{best_epoch}_self_attention.pt'))\n",
    "    writelog(f, 'Final Test')\n",
    "    #rmse, mae, \n",
    "    auc, auprc, acc, balacc, sen, spec, prec, recall = test('test', epoch, test_loader)\n",
    "\n",
    "    # KFold 결과 기록\n",
    "    kfold_auc.append(auc)\n",
    "    kfold_auprc.append(auprc)\n",
    "    kfold_acc.append(acc)\n",
    "    kfold_balacc.append(balacc)\n",
    "    kfold_sen.append(sen)\n",
    "    kfold_spec.append(spec)\n",
    "    kfold_precision.append(prec)\n",
    "    kfold_recall.append(recall)\n",
    "\n",
    "    # TensorBoard SummaryWriter 닫기\n",
    "    writer_train.close()\n",
    "    writer_valid.close()\n",
    "    writer_test.close()\n",
    "\n",
    "# KFold 성능 요약\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'SUMMARY OF ALL KFOLD')\n",
    "\n",
    "mean_auc = round(np.mean(kfold_auc), 5)\n",
    "std_auc = round(np.std(kfold_auc), 5)\n",
    "\n",
    "mean_auc_prc = round(np.mean(kfold_auprc), 5)\n",
    "std_auc_prc = round(np.std(kfold_auprc), 5)\n",
    "\n",
    "mean_acc = round(np.mean(kfold_acc), 5)\n",
    "std_acc = round(np.std(kfold_acc), 5)\n",
    "\n",
    "mean_balacc = round(np.mean(kfold_balacc), 5)\n",
    "std_balacc = round(np.std(kfold_balacc), 5)\n",
    "\n",
    "mean_sen = round(np.mean(kfold_sen), 5)\n",
    "std_sen = round(np.std(kfold_sen), 5)\n",
    "\n",
    "mean_spec = round(np.mean(kfold_spec), 5)\n",
    "std_spec = round(np.std(kfold_spec), 5)\n",
    "\n",
    "mean_precision = round(np.mean(kfold_precision), 5)\n",
    "std_precision = round(np.std(kfold_precision), 5)\n",
    "\n",
    "mean_recall = round(np.mean(kfold_recall), 5)\n",
    "std_recall = round(np.std(kfold_recall), 5)\n",
    "\n",
    "writelog(f, 'AUC : ' + str(mean_auc) + ' + ' + str(std_auc))\n",
    "writelog(f, 'AUC PRC : ' + str(mean_auc_prc) + ' + ' + str(std_auc_prc))\n",
    "writelog(f, 'Accuracy : ' + str(mean_acc) + ' + ' + str(std_acc))\n",
    "writelog(f, 'BalACC : ' + str(mean_balacc) + ' + ' + str(std_balacc))\n",
    "writelog(f, 'Sensitivity : ' + str(mean_sen) + ' + ' + str(std_sen))\n",
    "writelog(f, 'Specificity : ' + str(mean_spec) + ' + ' + str(std_spec))\n",
    "writelog(f, 'Precision : ' + str(mean_precision) + ' + ' + str(std_precision))\n",
    "writelog(f, 'Recall : ' + str(mean_recall) + ' + ' + str(std_recall))\n",
    "writelog(f, '---------------------')\n",
    "writelog(f, 'END OF CROSS VALIDATION TRAINING')\n",
    "f.close()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "144e5648-03bf-4853-88ee-b212d3303589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_auc :  0.82159\n",
      "std_auc :  0.02685\n"
     ]
    }
   ],
   "source": [
    "mean_auc = round(np.mean(kfold_auc), 5)\n",
    "std_auc = round(np.std(kfold_auc), 5)\n",
    "print(\"mean_auc : \",mean_auc)\n",
    "print(\"std_auc : \",std_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b3393f-132d-4bdd-ad4e-8ea8737cdfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_auc_prc :  0.44713\n",
      "std_auc_prc :  0.03198\n"
     ]
    }
   ],
   "source": [
    "mean_auc_prc = round(np.mean(kfold_auprc), 5)\n",
    "std_auc_prc = round(np.std(kfold_auprc), 5)\n",
    "print(\"mean_auc_prc : \",mean_auc_prc)\n",
    "print(\"std_auc_prc : \",std_auc_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e19443-adc0-464b-a92b-d1c224caa1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_acc :  0.86667\n",
      "std_acc :  0.00059\n"
     ]
    }
   ],
   "source": [
    "mean_acc = round(np.mean(kfold_acc), 5)\n",
    "std_acc = round(np.std(kfold_acc), 5)\n",
    "print(\"mean_acc : \",mean_acc)\n",
    "print(\"std_acc : \",std_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32595a17-639f-4daf-96d0-7a7760f26c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_balacc :  61.39899\n",
      "std_balacc :  2.07712\n"
     ]
    }
   ],
   "source": [
    "mean_balacc = round(np.mean(kfold_balacc), 5)\n",
    "std_balacc = round(np.std(kfold_balacc), 5)\n",
    "\n",
    "print(\"mean_balacc : \",mean_balacc)\n",
    "print(\"std_balacc : \",std_balacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184f079-2c0e-489d-96fe-e58ef2bd530f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e707c09-537b-4ec1-a6f6-0a002e8a7e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbac2f-1c86-4b97-8740-860056e678f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cb0b9-5941-4e10-abc5-e7fc873b4e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2130e-ea0c-49f7-8aa0-9222b550f17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f27063-53a5-40b6-b469-d42b53e410e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2f0ba-0b7a-42f8-be00-f308b493b647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc3f01-80f4-4231-a181-64c954358935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026e525-139f-41ca-96ff-976cefa4e350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebf858-a744-492c-a8a1-c7568eb32dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc738c3-ae70-4b1f-b93b-535fa9d6f20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5539ab-c928-4754-bf1b-84abda405445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miam",
   "language": "python",
   "name": "miam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
