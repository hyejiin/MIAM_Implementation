{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f47941c-3051-44ef-a528-2f1091461ac2",
   "metadata": {},
   "source": [
    "##  PyTorch implementation of the paper : Multi-view Integration Learning for Irregularly-sampled Clinical Time Series (MIAM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb6577-5572-4778-b1d5-0df0fccfe8f0",
   "metadata": {},
   "source": [
    "Using PhysioNet 2019 Challenge dataset, Early Sepsis Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ad00d-ced3-458e-a4ed-5758c0436a7d",
   "metadata": {},
   "source": [
    "You can download the dataset by entering the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "wget -r -N -c -np https://physionet.org/files/challenge-2019/1.0.0/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602cfec-b747-4bdd-b8d5-33275aaf5382",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adb030-76c9-43da-ada1-7466087293c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Data paths\n",
    "data_paths = [\n",
    "    '/media/usr/HDD/Data/EHR/physionet_challenge_2019/training/training_setA/',\n",
    "    '/media/usr/HDD/Data/EHR/physionet_challenge_2019/training/training_setB/'\n",
    "]\n",
    "\n",
    "# Variables to use\n",
    "variables = [\n",
    "    'HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', \n",
    "    'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', \n",
    "    'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', \n",
    "    'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', \n",
    "    'Phosphate', 'Potassium', 'Bilirubin_total', 'TroponinI', \n",
    "    'Hct', 'Hgb', 'PTT', 'WBC', 'Fibrinogen', 'Platelets'\n",
    "]\n",
    "\n",
    "# To store results\n",
    "all_patient_data = {}\n",
    "excluded_patients = []  # List of excluded patients (record_id and data length)\n",
    "\n",
    "# Process each folder\n",
    "for data_path in data_paths:\n",
    "    for file_name in sorted(os.listdir(data_path)):\n",
    "        if not file_name.endswith('.psv'):\n",
    "            continue\n",
    "\n",
    "        # Read file\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='|')\n",
    "\n",
    "        # Extract Record ID\n",
    "        record_id = file_name.split('.')[0]\n",
    "\n",
    "        # Check length and exclude if necessary\n",
    "        # if len(df) < 48:\n",
    "        #    excluded_patients.append({\"record_id\": record_id, \"length\": len(df)})\n",
    "        #    continue\n",
    "\n",
    "        # Select time steps and variables\n",
    "        df = df[:72]  # Limit to the first 72 hours\n",
    "        s_patient = df['ICULOS'].to_numpy()  # Time steps (s)\n",
    "        X_patient = df[variables].to_numpy()  # X (original data)\n",
    "\n",
    "        # M: Masking vector (indicates missing values)\n",
    "        M_patient = (~np.isnan(X_patient)).astype(int)\n",
    "\n",
    "        # Delta: Time elapsed since the last observation for each variable\n",
    "        Delta_patient = np.zeros_like(X_patient, dtype=float)\n",
    "        for t in range(1, len(s_patient)):\n",
    "            delta_t = s_patient[t] - s_patient[t - 1]  # Current time step interval\n",
    "            Delta_patient[t] = Delta_patient[t - 1] + delta_t  # Accumulated time\n",
    "            Delta_patient[t][M_patient[t] == 1] = delta_t  # Reset for observed values\n",
    "\n",
    "        # Save data in all_patient_data\n",
    "        all_patient_data[record_id] = {\n",
    "            \"X\": X_patient,\n",
    "            \"M\": M_patient,\n",
    "            \"Delta\": Delta_patient,\n",
    "            \"s\": s_patient\n",
    "        }\n",
    "\n",
    "# Convert all_patient_data to indexed_patient_data\n",
    "record_id_to_index = {record_id: idx for idx, record_id in enumerate(all_patient_data.keys())}\n",
    "index_to_record_id = {idx: record_id for record_id, idx in record_id_to_index.items()}\n",
    "\n",
    "indexed_patient_data = [\n",
    "    all_patient_data[record_id] for record_id in record_id_to_index.keys()\n",
    "]\n",
    "\n",
    "# Compare dataset statistics\n",
    "total_patients = sum(len([f for f in os.listdir(data_path) if f.endswith('.psv')]) for data_path in data_paths)\n",
    "excluded_count = len(excluded_patients)\n",
    "processed_count = len(indexed_patient_data)\n",
    "\n",
    "print(f\"Total number of patient files (data_path): {total_patients}\")\n",
    "print(f\"Number of excluded patients: {excluded_count}\")\n",
    "print(f\"Number of processed patients (indexed_patient_data): {processed_count}\")\n",
    "print(f\"Validation: {total_patients == excluded_count + processed_count}\")\n",
    "\n",
    "# Save data if necessary\n",
    "output_path = '/media/usr/HDD/Data/EHR/indexed_patient_data.npz'\n",
    "np.savez_compressed(output_path, indexed_patient_data=indexed_patient_data)\n",
    "\n",
    "mapping_path = '/media/usr/HDD/Data/EHR/record_id_to_index.json'\n",
    "with open(mapping_path, 'w') as f:\n",
    "    json.dump(record_id_to_index, f)\n",
    "\n",
    "excluded_path = '/media/usr/HDD/Data/EHR/excluded_patients.csv'\n",
    "pd.DataFrame(excluded_patients).to_csv(excluded_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6eee0d4e-944d-4582-9460-082e7eb8e057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "        [ 97.,  95.,  nan, ...,  nan,  nan,  nan],\n",
       "        [ 89.,  99.,  nan, ...,  nan,  nan,  nan],\n",
       "        ...,\n",
       "        [ 85., 100.,  nan, ...,  nan,  nan,  nan],\n",
       "        [ 86.,  93.,  nan, ...,  nan,  nan,  nan],\n",
       "        [ 84.,  85.,  nan, ...,  nan,  nan,  nan]]),\n",
       " 'M': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 0, ..., 0, 0, 0]]),\n",
       " 'Delta': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  2., ...,  2.,  2.,  2.],\n",
       "        ...,\n",
       "        [ 1.,  1., 11., ..., 13., 51., 13.],\n",
       "        [ 1.,  1., 12., ..., 14., 52., 14.],\n",
       "        [ 1.,  1., 13., ..., 15., 53., 15.]]),\n",
       " 's': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "        35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "        52, 53, 54])}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_patient_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b38ed-5a6b-4b2d-811c-5b7ed39e969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# List of data paths\n",
    "data_paths = [\n",
    "    '/media/usr/HDD/Data/EHR/physionet_challenge_2019/training/training_setA/',\n",
    "    '/media/usr/HDD/Data/EHR/physionet_challenge_2019/training/training_setB/'\n",
    "]\n",
    "record_id_to_index_path = '/media/usr/HDD/Data/EHR/record_id_to_index.json'\n",
    "\n",
    "# Load record_id_to_index mapping\n",
    "with open(record_id_to_index_path, 'r') as f:\n",
    "    record_id_to_index = json.load(f)\n",
    "\n",
    "# To store results\n",
    "patient_sepsis_labels = []\n",
    "\n",
    "# Process all folders\n",
    "for data_path in data_paths:\n",
    "    for file_name in sorted(os.listdir(data_path)):\n",
    "        if not file_name.endswith('.psv'):\n",
    "            continue\n",
    "\n",
    "        # Read file\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='|')\n",
    "\n",
    "        # Extract Record ID\n",
    "        record_id = file_name.split('.')[0]\n",
    "\n",
    "        # Skip PatientIDs not in record_id_to_index\n",
    "        if record_id not in record_id_to_index:\n",
    "            continue\n",
    "\n",
    "        # Check SepsisLabel values\n",
    "        if 'SepsisLabel' in df.columns:\n",
    "            sepsis_label = 1 if df['SepsisLabel'].sum() > 0 else 0  # Check if SepsisLabel contains any '1'\n",
    "        else:\n",
    "            sepsis_label = 0  # Default value if SepsisLabel column is missing\n",
    "\n",
    "        # Save result\n",
    "        patient_sepsis_labels.append({'PatientID': record_id, 'SepsisLabel': sepsis_label})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "sepsis_labels_df = pd.DataFrame(patient_sepsis_labels)\n",
    "\n",
    "# Save results to a file\n",
    "output_path = '/media/usr/HDD/Data/EHR/patient_sepsis_labels_filtered.csv'\n",
    "sepsis_labels_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "81c4c7f4-50b5-453e-baf4-fa33c02c2804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PatientID\n",
       "SepsisLabel           \n",
       "0                37404\n",
       "1                 2932"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_labels_df.groupby('SepsisLabel').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c522fd-6d57-43b2-91fb-2060e1daa72f",
   "metadata": {},
   "source": [
    "### Store all patients' X, M, Delta, s data in a structured format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "952085c5-4ac1-48ff-8e75-8a3fa334a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store all patients' X, M, Delta, s data in a structured format\n",
    "num_patients = len(record_id_to_index)\n",
    "\n",
    "# Prepare lists to hold each array for all patients\n",
    "X = []\n",
    "M = []\n",
    "Delta = []\n",
    "s = []\n",
    "\n",
    "# Loop through each patient to populate the arrays\n",
    "for record_id in record_id_to_index.keys():\n",
    "    patient_data = all_patient_data[record_id]\n",
    "    X.append(patient_data['X'])\n",
    "    M.append(patient_data['M'])\n",
    "    Delta.append(patient_data['Delta'])\n",
    "    s.append(patient_data['s'])\n",
    "\n",
    "# Convert lists to arrays for structured storage if needed\n",
    "#X = np.array(X)      # Shape: (num_patients, num_time_steps, num_variables)\n",
    "#M = np.array(M)     # Shape: (num_patients, num_time_steps, num_variables)\n",
    "#Delta = np.array(Delta)  # Shape: (num_patients, num_time_steps, num_variables)\n",
    "#s = np.array(s)     # Shape: (num_patients, num_time_steps)\n",
    "\n",
    "# Confirm the shape of each array for verification\n",
    "#X.shape, M.shape, Delta.shape, s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b0a5a892-fc73-497b-9126-194a3510cf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 40336\n",
      "First patient's X shape: (54, 33)\n",
      "First patient's M shape: (54, 33)\n",
      "First patient's Delta shape: (54, 33)\n",
      "First patient's s shape: (54,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patients: {len(X)}\")\n",
    "print(f\"First patient's X shape: {X[0].shape}\")\n",
    "print(f\"First patient's M shape: {M[0].shape}\")\n",
    "print(f\"First patient's Delta shape: {Delta[0].shape}\")\n",
    "print(f\"First patient's s shape: {s[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a3d23d00-168a-46ae-9058-ea5c1ff072f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (40336, 72, 33)\n",
      "M shape: (40336, 72, 33)\n",
      "Delta shape: (40336, 72, 33)\n",
      "s shape: (40336, 72)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the fixed number of time steps\n",
    "max_time_steps = 72  # Change this to the desired fixed length\n",
    "\n",
    "# Prepare lists for padded/truncated data\n",
    "X_fixed = []\n",
    "M_fixed = []\n",
    "Delta_fixed = []\n",
    "s_fixed = []\n",
    "\n",
    "# Process each patient\n",
    "for record_id in record_id_to_index.keys():\n",
    "    patient_data = all_patient_data[record_id]\n",
    "    x = patient_data['X']\n",
    "    m = patient_data['M']\n",
    "    delta = patient_data['Delta']\n",
    "    s = patient_data['s']\n",
    "    \n",
    "    # Determine the current length\n",
    "    current_length = len(x)\n",
    "    \n",
    "    if current_length < max_time_steps:\n",
    "        # Padding: Add zeros to the end\n",
    "        padding_length = max_time_steps - current_length\n",
    "        x_padded = np.pad(x, ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "        m_padded = np.pad(m, ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "        delta_padded = np.pad(delta, ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "        s_padded = np.pad(s, (0, padding_length), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        # Truncate: Keep only the first max_time_steps entries\n",
    "        x_padded = x[:max_time_steps]\n",
    "        m_padded = m[:max_time_steps]\n",
    "        delta_padded = delta[:max_time_steps]\n",
    "        s_padded = s[:max_time_steps]\n",
    "    \n",
    "    # Append the processed data\n",
    "    X_fixed.append(x_padded)\n",
    "    M_fixed.append(m_padded)\n",
    "    Delta_fixed.append(delta_padded)\n",
    "    s_fixed.append(s_padded)\n",
    "\n",
    "# Convert to numpy arrays with consistent shapes\n",
    "X = np.array(X_fixed)       # Shape: (num_patients, max_time_steps, num_variables)\n",
    "M = np.array(M_fixed)       # Shape: (num_patients, max_time_steps, num_variables)\n",
    "Delta = np.array(Delta_fixed)  # Shape: (num_patients, max_time_steps, num_variables)\n",
    "s = np.array(s_fixed)       # Shape: (num_patients, max_time_steps)\n",
    "\n",
    "# Confirm the shape\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"M shape: {M.shape}\")\n",
    "print(f\"Delta shape: {Delta.shape}\")\n",
    "print(f\"s shape: {s.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae9a7a-77ce-4f66-956e-864f49547a3a",
   "metadata": {},
   "source": [
    "### kfold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b5c9073-6d0c-45bc-a818-aaa9e08d92d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kfold_X[0][0] (train)': (24184, 72, 33),\n",
       " 'kfold_X[0][1] (validate)': (8076, 72, 33),\n",
       " 'kfold_X[0][2] (test)': (8076, 72, 33),\n",
       " 'kfold_label[0][0] (train label)': (24184,),\n",
       " 'kfold_label[0][1] (validate label)': (8076,),\n",
       " 'kfold_label[0][2] (test label)': (8076,)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "# Prepare labels for stratified splitting\n",
    "label = sepsis_labels_df['SepsisLabel'].values\n",
    "\n",
    "# Adjust label to have an 8:2 ratio for stratified splitting\n",
    "# Assuming binary classification, we can convert label to categorical with 0 (negative) and 1 (positive)\n",
    "# We will manually create the stratification based on this label\n",
    "positive_indices = np.where(label == 1)[0]\n",
    "negative_indices = np.where(label == 0)[0]\n",
    "\n",
    "# Calculate how many positive and negative samples to include in each fold\n",
    "num_positive = int(len(positive_indices) * 0.8)  # 80% positive for training\n",
    "num_negative = int(len(negative_indices) * 0.8)  # 80% negative for training\n",
    "\n",
    "# Initialize the k-fold split containers\n",
    "num_folds = 4\n",
    "kfold_X = [[] for _ in range(num_folds)]\n",
    "kfold_M = [[] for _ in range(num_folds)]\n",
    "kfold_Delta = [[] for _ in range(num_folds)]\n",
    "kfold_s = [[] for _ in range(num_folds)]\n",
    "kfold_label = [[] for _ in range(num_folds)]\n",
    "kfold_label_2 = [[] for _ in range(num_folds)]\n",
    "\n",
    "# StratifiedShuffleSplit to create train, validate, and test splits\n",
    "outer_splitter = StratifiedShuffleSplit(n_splits=num_folds, test_size=0.2002, random_state=128) \n",
    "\n",
    "for fold_idx, (train_val_index, test_index) in enumerate(outer_splitter.split(np.zeros(len(label)), label)):\n",
    "    # Split the remaining 80% into train (70%) and validate (10%)\n",
    "    inner_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2002 / 0.7998, random_state=128)  \n",
    "    train_index, val_index = next(inner_splitter.split(np.zeros(len(train_val_index)), label[train_val_index]))\n",
    "\n",
    "    # Map indices back to the original array\n",
    "    train_index = train_val_index[train_index]\n",
    "    val_index = train_val_index[val_index]\n",
    "\n",
    "    # Assign train, validate, and test data for each fold\n",
    "    kfold_X[fold_idx] = [X[train_index], X[val_index], X[test_index]]\n",
    "    kfold_M[fold_idx] = [M[train_index], M[val_index], M[test_index]]\n",
    "    kfold_Delta[fold_idx] = [Delta[train_index], Delta[val_index], Delta[test_index]]\n",
    "    kfold_s[fold_idx] = [s[train_index], s[val_index], s[test_index]]\n",
    "    kfold_label[fold_idx] = [label[train_index], label[val_index], label[test_index]]\n",
    "\n",
    "# Output structure of the first fold to confirm\n",
    "{\n",
    "    \"kfold_X[0][0] (train)\": kfold_X[0][0].shape,\n",
    "    \"kfold_X[0][1] (validate)\": kfold_X[0][1].shape,\n",
    "    \"kfold_X[0][2] (test)\": kfold_X[0][2].shape,\n",
    "    \"kfold_label[0][0] (train label)\": kfold_label[0][0].shape,\n",
    "    \"kfold_label[0][1] (validate label)\": kfold_label[0][1].shape,\n",
    "    \"kfold_label[0][2] (test label)\": kfold_label[0][2].shape\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773320b0-5e44-4831-87ef-8593f7716ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a structure to hold missing rates and label distributions for each fold\n",
    "fold_summary = {}\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    # Extract the data for the current fold\n",
    "    train_data = kfold_X[fold_idx][0]\n",
    "    val_data = kfold_X[fold_idx][1]\n",
    "    test_data = kfold_X[fold_idx][2]\n",
    "    \n",
    "    train_labels = kfold_label[fold_idx][0]\n",
    "    val_labels = kfold_label[fold_idx][1]\n",
    "    test_labels = kfold_label[fold_idx][2]\n",
    "\n",
    "    # Calculate missing rates for train, validate, and test datasets\n",
    "    train_missing_rate = np.mean(np.isnan(train_data))\n",
    "    val_missing_rate = np.mean(np.isnan(val_data))\n",
    "    test_missing_rate = np.mean(np.isnan(test_data))\n",
    "\n",
    "    # Calculate label distributions for train, validate, and test datasets\n",
    "    train_positive_ratio = np.sum(train_labels == 1) / len(train_labels)\n",
    "    val_positive_ratio = np.sum(val_labels == 1) / len(val_labels)\n",
    "    test_positive_ratio = np.sum(test_labels == 1) / len(test_labels)\n",
    "\n",
    "\n",
    "    # Store the results for the current fold\n",
    "    fold_summary[fold_idx] = {\n",
    "        \"train_missing_rate\": train_missing_rate,\n",
    "        \"val_missing_rate\": val_missing_rate,\n",
    "        \"test_missing_rate\": test_missing_rate,\n",
    "        \"train_positive_ratio\": train_positive_ratio,\n",
    "        \"val_positive_ratio\": val_positive_ratio,\n",
    "        \"test_positive_ratio\": test_positive_ratio,\n",
    "    }\n",
    "\n",
    "# Display the summary for the first fold\n",
    "#fold_summary  # Showing for fold 0, can be changed to other fold indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "364720f7-f299-4252-8866-bb17ab3a947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kfold_data_chellenge2019.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'kfold_X': kfold_X,\n",
    "        'kfold_M': kfold_M,\n",
    "        'kfold_Delta': kfold_Delta,\n",
    "        'kfold_s': kfold_s,\n",
    "        'kfold_label': kfold_label,\n",
    "    }, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5f5da-f624-4536-b421-6676bfffe08e",
   "metadata": {},
   "source": [
    "### Task : Sepsis Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a67c9-33b4-4ca9-8bd3-a91c02451619",
   "metadata": {},
   "source": [
    "Need to import\n",
    "- model.py\n",
    "- help_physionet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "39822359-3651-49c2-97b4-f87cc4c48fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout zero, relu\n",
      "Using GPU ID 0 if available, else CPU\n",
      "Assigned device: cuda:0\n",
      "focal(y): 0 , mse(x): 1\n",
      "---------------\n",
      "MIAM\n",
      "Dataset :/media/usr/HDD/hyejin/MIAM\n",
      "---------------\n",
      "TRAINING PARAMETER\n",
      "Learning Rate : 0.0005\n",
      "LR decay : 0.1\n",
      "Batch Size : 64\n",
      "lambda1 : 0.0005\n",
      "---------------\n",
      "Transformer Setup\n",
      "hidden_dim : 64\n",
      "FFN_dim : 64\n",
      "num_heads : 4\n",
      "num_stacks : 1\n",
      "---------------\n",
      "Loss Setup\n",
      "cls:0, reg:1, imp:1\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import argparse\n",
    "import warnings\n",
    "import random\n",
    "from help_physionet import *\n",
    "from models import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torchstat import stat\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"GEVENT_SUPPORT\"] = \"True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "torch.backends.cudnn.enabled = False\n",
    "JOBLIB_MULTIPROCESSING=1\n",
    "\n",
    "# Define Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", help=\"which dataset to use\", type=str, default='physionet')\n",
    "parser.add_argument('--fold_num', type=int, default=0)\n",
    "parser.add_argument('--l1', type=float, default=5e-4)\n",
    "parser.add_argument('--w_decay', type=float, default=5e-3)#1e-3)\n",
    "parser.add_argument('--lr', type=float, default=5e-4)#5e-3)\n",
    "parser.add_argument('--lr_decay', type=int, default=15)\n",
    "parser.add_argument('--lr_ratio', type=float, default=0.1)\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "\n",
    "\n",
    "print('dropout zero, relu')\n",
    "args, unknown = parser.parse_known_args() \n",
    "dataset = args.dataset\n",
    "fold_num = args.fold_num\n",
    "l1 = args.l1\n",
    "w_decay = args.w_decay\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "lr_decay = args.lr_decay\n",
    "lr_ratio = args.lr_ratio\n",
    "\n",
    "# Set the GPU configuration\n",
    "device_number = args.gpu_id\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(device_number)\n",
    "dev_allo = f\"cuda:{device_number}\" if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(dev_allo)\n",
    "\n",
    "print(f'Using GPU ID {device_number} if available, else CPU')\n",
    "print(f'Assigned device: {device}')\n",
    "\n",
    "# Load Kfold dataset\n",
    "data_dir = '/media/usr/HDD/hyejin/MIAM'\n",
    "\n",
    "with open('kfold_data_chellenge2019.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "kfold_data = data['kfold_X']\n",
    "kfold_mask = data['kfold_M']\n",
    "kfold_times = data['kfold_s']\n",
    "kfold_label = data['kfold_label']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "n_epochs = 60\n",
    "alpha = 9\n",
    "gamma = 0.15\n",
    "beta = 0.1\n",
    "delta = 11\n",
    "# Loss rates\n",
    "lambda_1 = 0\n",
    "lambda_2 = 1\n",
    "lambda_3 = 1\n",
    "print('focal(y):', str(lambda_1), ', mse(x):', str(lambda_2))\n",
    "KFold = len(kfold_data)\n",
    "\n",
    "# Network architecture\n",
    "max_length = kfold_data[0][0].shape[1]\n",
    "input_dim = kfold_data[0][0].shape[2]\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 64\n",
    "num_stacks = 1\n",
    "num_heads = 4\n",
    "\n",
    "# Seed\n",
    "manualSeed = 128\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "# kfold performance\n",
    "kfold_mse = []\n",
    "kfold_mae = []\n",
    "kfold_acc = []\n",
    "kfold_balacc = []\n",
    "kfold_auc = []\n",
    "kfold_auprc = []\n",
    "kfold_sen = []\n",
    "kfold_spec = []\n",
    "kfold_precision = []\n",
    "kfold_recall = []\n",
    "kfold_f1_score_pr = []\n",
    "kfold_f2_score_pr = []\n",
    "\n",
    "\n",
    "def switch(fold_num):\n",
    "    return {0: range(0, 1),\n",
    "            1: range(1, 2),\n",
    "            2: range(2, 3),\n",
    "            3: range(3, 4),\n",
    "            4: range(4, 5)}[fold_num]\n",
    "\n",
    "\n",
    "# Create Directories\n",
    "log_dir = './log/' + str(datetime.datetime.now().strftime('%y%m%d')) + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    os.chmod(log_dir, mode=0o777)\n",
    "dir = log_dir + 'observation_mask_multi_encoder_' + str(datetime.datetime.now().strftime('%H.%M.%S')) + '/'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "    os.makedirs(dir + 'model/')\n",
    "    os.makedirs(dir + 'tflog/')\n",
    "    for k in range(KFold):\n",
    "        os.makedirs(dir + 'model/' + str(k) + '/')\n",
    "\n",
    "# TensorBoard Logging Setup\n",
    "writer = SummaryWriter(log_dir=dir + 'tflog')\n",
    "\n",
    "# Text Logging\n",
    "f = open(dir + 'log.txt', 'a')\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'MIAM')\n",
    "writelog(f, 'Dataset :' + str(data_dir))\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'TRAINING PARAMETER')\n",
    "writelog(f, 'Learning Rate : ' + str(lr))\n",
    "writelog(f, 'LR decay : '+ str(lr_ratio))\n",
    "writelog(f, 'Batch Size : ' + str(batch_size))\n",
    "writelog(f, 'lambda1 : ' + str(l1))\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'Transformer Setup')\n",
    "writelog(f, 'hidden_dim : ' + str(d_model))\n",
    "writelog(f, 'FFN_dim : ' + str(d_ff))\n",
    "writelog(f, 'num_heads : ' + str(num_heads))\n",
    "writelog(f, 'num_stacks : ' + str(num_stacks))\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'Loss Setup')\n",
    "writelog(f, 'cls:'+ str(lambda_1) + ', reg:' + str(lambda_2) +', imp:'+ str(lambda_3))\n",
    "writelog(f, '---------------')\n",
    "\n",
    "def train(epoch, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x = data['values'].to(device)  # Batch x Time x Variable\n",
    "        m = data['masks'].to(device)  # Batch x Time x Variable\n",
    "        deltas = data['deltas'].to(device)  # Batch x Time x Variable\n",
    "        times = data['times'].to(device)  # Batch x Time x Variable\n",
    "        y = data['labels'].to(device)\n",
    "\n",
    "        attn_mask = deltas.data.eq(0)[:, :, 0]\n",
    "        attn_mask[:, 0] = 0\n",
    "\n",
    "        # Zero Grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # model\n",
    "        output, out = model(x, m, times, deltas, attn_mask)\n",
    "\n",
    "        # Calculate and store the loss\n",
    "        loss_a = criterion_focal(model, output, y)\n",
    "        loss_b = criterion_mse(out, x)\n",
    "        loss = beta*loss_a + delta*loss_b\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = train_loss / n_batches\n",
    "    writelog(f, 'Train loss : ' + str(train_loss))\n",
    "\n",
    "\n",
    "def test(phase, epoch, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    n_batches = 0.0\n",
    "\n",
    "    y_gts = np.array([]).reshape(0)\n",
    "    y_preds = np.array([]).reshape(0)\n",
    "    y_scores = np.array([]).reshape(0)\n",
    "\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        x = data['values'].to(device)  # Batch x Time x Variable\n",
    "        m = data['masks'].to(device)  # Batch x Time x Variable\n",
    "        deltas = data['deltas'].to(device)  # Batch x Time x Variable\n",
    "        times = data['times'].to(device)  # Batch x Time x Variable\n",
    "        y = data['labels'].to(device)\n",
    "\n",
    "        attn_mask = deltas.data.eq(0)[:, :, 0]\n",
    "        attn_mask[:, 0] = 0\n",
    "\n",
    "        y_gts = np.hstack([y_gts, y.to('cpu').detach().numpy().flatten()]) #physionet\n",
    "\n",
    "        # model\n",
    "        output, out = model(x, m, times, deltas, attn_mask)\n",
    "\n",
    "        # Calculate and store the loss\n",
    "        loss_a = criterion_focal(model, output, y)\n",
    "        loss_b = criterion_mse(out, x)\n",
    "        loss = loss_a #beta*loss_a + delta*loss_b\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        y_score = output\n",
    "        y_pred = np.round(y_score.to('cpu').detach().numpy())\n",
    "        y_score = y_score.to('cpu').detach().numpy()\n",
    "        y_preds = np.hstack([y_preds, y_pred])\n",
    "        y_scores = np.hstack([y_scores, y_score])\n",
    "\n",
    "        n_batches += 1\n",
    "\n",
    "    # Averaging the loss\n",
    "    test_loss /= n_batches\n",
    "    writelog(f, 'Test loss : ' + str(test_loss))\n",
    "\n",
    "    auc, auprc, acc, balacc, sen, spec, prec, recall = calculate_performance(y_gts, y_scores, y_preds)\n",
    "\n",
    "    writelog(f, 'AUC : ' + str(auc))\n",
    "    writelog(f, 'AUC PRC : ' + str(auprc))\n",
    "    writelog(f, 'Accuracy : ' + str(acc))\n",
    "    writelog(f, 'BalACC : ' + str(balacc))\n",
    "    writelog(f, 'Sensitivity : ' + str(sen))\n",
    "    writelog(f, 'Specificity : ' + str(spec))\n",
    "    writelog(f, 'Precision : ' + str(prec))\n",
    "    writelog(f, 'Recall : ' + str(recall))\n",
    "\n",
    "        # TensorBoard Logging\n",
    "    writer.add_scalars(f'Metrics/{phase}', {\n",
    "        'balacc': balacc,\n",
    "        'auc': auc,\n",
    "        'auc_prc': auprc,\n",
    "        'sens': sen,\n",
    "        'spec': spec,\n",
    "        'precision': prec,\n",
    "        'recall': recall\n",
    "    }, epoch)\n",
    "\n",
    "    return auc, auprc, acc, balacc, sen, spec, prec, recall\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7694ead-3ca6-470e-ad3b-cac91b45514b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# KFold 반복 루프\n",
    "for k in range(KFold):\n",
    "    writelog(f, 'FOLD ' + str(k))\n",
    "\n",
    "    # TensorBoard Logging을 위한 SummaryWriter 설정\n",
    "    writer_train = SummaryWriter(log_dir=dir + f'tflog/kfold_{k}/train')\n",
    "    writer_valid = SummaryWriter(log_dir=dir + f'tflog/kfold_{k}/valid')\n",
    "    writer_test = SummaryWriter(log_dir=dir + f'tflog/kfold_{k}/test')\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    train_data = kfold_data[k][0]\n",
    "    train_mask = kfold_mask[k][0]\n",
    "    tr_miss_idx = np.where(train_mask == 0)\n",
    "    train_data[tr_miss_idx] = 0\n",
    "    train_label = kfold_label[k][0]\n",
    "    train_time = kfold_times[k][0]\n",
    "\n",
    "    valid_data = kfold_data[k][1]\n",
    "    valid_mask = kfold_mask[k][1]\n",
    "    val_miss_idx = np.where(valid_mask == 0)\n",
    "    valid_data[val_miss_idx] = 0\n",
    "    valid_label = kfold_label[k][1]\n",
    "    valid_time = kfold_times[k][1]\n",
    "\n",
    "    test_data = kfold_data[k][2]\n",
    "    test_mask = kfold_mask[k][2]\n",
    "    ts_miss_idx = np.where(test_mask == 0)\n",
    "    test_data[ts_miss_idx] = 0\n",
    "    test_label = kfold_label[k][2]\n",
    "    test_time = kfold_times[k][2]\n",
    "    \n",
    "   \n",
    "    # Winsorization (2nd-98th percentile)\n",
    "    writelog(f, 'Winsorization')\n",
    "    train_data = Winsorize(train_data)\n",
    "    valid_data = Winsorize(valid_data)\n",
    "    test_data = Winsorize(test_data)\n",
    "    \n",
    "\n",
    "    # # Normalization\n",
    "    writelog(f, 'Normalization')\n",
    "    train_data, mean_set, std_set = normalize(train_data, train_mask, [], [])\n",
    "    valid_data, m, s = normalize(valid_data, valid_mask, mean_set, std_set)\n",
    "    test_data, m, s = normalize(test_data, test_mask, mean_set, std_set)\n",
    "    \n",
    "    \n",
    "    test_data_zero = test_data.copy()\n",
    "    test_data_zero[ts_miss_idx] = 0  # zero imputation\n",
    "    test_ms_data_zero, test_data_zero, test_msk= random_mask(test_data_zero)\n",
    "\n",
    "\n",
    "    # 데이터 로더 정의\n",
    "    train_loader = sample_loader('train', k, train_data, train_mask, train_label, train_time, batch_size, ZeroImpute=True)\n",
    "    valid_loader = sample_loader('valid', k, valid_data, valid_mask, valid_label, valid_time, batch_size, ZeroImpute=True)\n",
    "    test_loader =  msk_sample_loader('test', k, test_data, test_mask, test_ms_data_zero, test_msk, test_label, test_time, batch_size, ZeroImpute=True)\n",
    "   \n",
    "\n",
    "    # 모델 및 옵티마이저 정의\n",
    "    criterion_focal = FocalLoss(l1, device, gamma=gamma, alpha=alpha, logits=False).to(device)\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    model = Multi_Duration_Pipeline_Residual(input_dim, d_model, d_ff, num_stacks, num_heads, max_length, n_iter=num_stacks).to(device)\n",
    "    \n",
    "    optimizer = RAdam(list(model.parameters()), lr=lr, weight_decay=w_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay, gamma=lr_ratio)\n",
    "\n",
    "    # Best Validation AUC 초기화\n",
    "    bestValidAUC = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # 훈련, 검증, 테스트 루프\n",
    "    for epoch in range(n_epochs):\n",
    "        writelog(f, '------ Epoch ' + str(epoch))\n",
    "\n",
    "        writelog(f, 'Training')\n",
    "        train(epoch, train_loader)\n",
    "\n",
    "        writelog(f, 'Validation')\n",
    "        #rmse, mae, \n",
    "        auc, auprc, acc, balacc, sen, spec, prec, recall = test('valid', epoch, valid_loader)\n",
    "\n",
    "        # 최적 AUC 모델 저장\n",
    "        if auc > bestValidAUC:\n",
    "            torch.save(model.state_dict(), dir + f'model/{k}/{epoch}_self_attention.pt')\n",
    "            writelog(f, 'Best validation AUC found! Validation AUC : ' + str(auc))\n",
    "            bestValidAUC = auc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        writelog(f, 'Test')\n",
    "        #rmse, mae, \n",
    "        auc, auprc, acc, balacc, sen, spec, prec, recall = test('test', epoch, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        # TensorBoard에 성능 기록\n",
    "        writer_train.add_scalar('AUC/train', auc, epoch)\n",
    "        writer_valid.add_scalar('AUC/valid', auc, epoch)\n",
    "        writer_test.add_scalar('AUC/test', auc, epoch)\n",
    "\n",
    "    # Best Validation 모델 로드 및 최종 테스트\n",
    "    model.load_state_dict(torch.load(dir + f'model/{k}/{best_epoch}_self_attention.pt'))\n",
    "    writelog(f, 'Final Test')\n",
    "    #rmse, mae, \n",
    "    auc, auprc, acc, balacc, sen, spec, prec, recall = test('test', epoch, test_loader)\n",
    "\n",
    "    # KFold 결과 기록\n",
    "    kfold_auc.append(auc)\n",
    "    kfold_auprc.append(auprc)\n",
    "    kfold_acc.append(acc)\n",
    "    kfold_balacc.append(balacc)\n",
    "    kfold_sen.append(sen)\n",
    "    kfold_spec.append(spec)\n",
    "    kfold_precision.append(prec)\n",
    "    kfold_recall.append(recall)\n",
    "\n",
    "    # TensorBoard SummaryWriter 닫기\n",
    "    writer_train.close()\n",
    "    writer_valid.close()\n",
    "    writer_test.close()\n",
    "\n",
    "# KFold 성능 요약\n",
    "writelog(f, '---------------')\n",
    "writelog(f, 'SUMMARY OF ALL KFOLD')\n",
    "\n",
    "mean_auc = round(np.mean(kfold_auc), 5)\n",
    "std_auc = round(np.std(kfold_auc), 5)\n",
    "\n",
    "mean_auc_prc = round(np.mean(kfold_auprc), 5)\n",
    "std_auc_prc = round(np.std(kfold_auprc), 5)\n",
    "\n",
    "mean_acc = round(np.mean(kfold_acc), 5)\n",
    "std_acc = round(np.std(kfold_acc), 5)\n",
    "\n",
    "mean_balacc = round(np.mean(kfold_balacc), 5)\n",
    "std_balacc = round(np.std(kfold_balacc), 5)\n",
    "\n",
    "mean_sen = round(np.mean(kfold_sen), 5)\n",
    "std_sen = round(np.std(kfold_sen), 5)\n",
    "\n",
    "mean_spec = round(np.mean(kfold_spec), 5)\n",
    "std_spec = round(np.std(kfold_spec), 5)\n",
    "\n",
    "mean_precision = round(np.mean(kfold_precision), 5)\n",
    "std_precision = round(np.std(kfold_precision), 5)\n",
    "\n",
    "mean_recall = round(np.mean(kfold_recall), 5)\n",
    "std_recall = round(np.std(kfold_recall), 5)\n",
    "\n",
    "writelog(f, 'AUC : ' + str(mean_auc) + ' + ' + str(std_auc))\n",
    "writelog(f, 'AUROC : ' + str(mean_auc) + ' + ' + str(std_auc))\n",
    "writelog(f, 'AUC PRC : ' + str(mean_auc_prc) + ' + ' + str(std_auc_prc))\n",
    "writelog(f, 'Accuracy : ' + str(mean_acc) + ' + ' + str(std_acc))\n",
    "writelog(f, 'BalACC : ' + str(mean_balacc) + ' + ' + str(std_balacc))\n",
    "writelog(f, 'Sensitivity : ' + str(mean_sen) + ' + ' + str(std_sen))\n",
    "writelog(f, 'Specificity : ' + str(mean_spec) + ' + ' + str(std_spec))\n",
    "writelog(f, 'Precision : ' + str(mean_precision) + ' + ' + str(std_precision))\n",
    "writelog(f, 'Recall : ' + str(mean_recall) + ' + ' + str(std_recall))\n",
    "writelog(f, '---------------------')\n",
    "writelog(f, 'END OF CROSS VALIDATION TRAINING')\n",
    "f.close()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "558b6900-4b6c-47d9-b5ef-eadcf8d53c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_auc :  0.9243\n",
      "std_auc :  0.00492\n"
     ]
    }
   ],
   "source": [
    "mean_auc = round(np.mean(kfold_auc), 5)\n",
    "std_auc = round(np.std(kfold_auc), 5)\n",
    "print(\"mean_auc : \",mean_auc)\n",
    "print(\"std_auc : \",std_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3616894a-2510-4296-948f-fd718b2f9be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_auc_prc :  0.71429\n",
      "std_auc_prc :  0.00934\n"
     ]
    }
   ],
   "source": [
    "mean_auc_prc = round(np.mean(kfold_auprc), 5)\n",
    "std_auc_prc = round(np.std(kfold_auprc), 5)\n",
    "print(\"mean_auc_prc : \",mean_auc_prc)\n",
    "print(\"std_auc_prc : \",std_auc_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d5b5d8e6-9428-45fe-916c-b554fac02221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_acc :  0.95666\n",
      "std_acc :  0.00045\n"
     ]
    }
   ],
   "source": [
    "mean_acc = round(np.mean(kfold_acc), 5)\n",
    "std_acc = round(np.std(kfold_acc), 5)\n",
    "print(\"mean_acc : \",mean_acc)\n",
    "print(\"std_acc : \",std_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f557b00d-16f5-4fcd-9530-dc33a844f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_balacc :  76.21245\n",
      "std_balacc :  0.64392\n"
     ]
    }
   ],
   "source": [
    "mean_balacc = round(np.mean(kfold_balacc), 5)\n",
    "std_balacc = round(np.std(kfold_balacc), 5)\n",
    "\n",
    "print(\"mean_balacc : \",mean_balacc)\n",
    "print(\"std_balacc : \",std_balacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5539ab-c928-4754-bf1b-84abda405445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miam",
   "language": "python",
   "name": "miam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
